{
  "metadata": {
    "name": "News Data  analysis final notebook",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Data cleaning and profiling for U.S. equities news data\nThe dataset used for this profiling is data of financial news related to U.S. equities. The aim of this notebook is to do data profiling, cleaning, and ingestion. The news dataset is obtained from Kaggle platform at : [Link to dataset on Kaggle](https://www.kaggle.com/datasets/gennadiyr/us-equities-news-data)\n\n## Acknowledgements\nThe origial datasource is from https://www.investing.com/ . Investing.com is an online data and news website that provides financial information. Every row of this dataset includes attribution to the data provider and link on the source.\n\n\u003cimg src\u003d\"https://i-invdn-com.investing.com/logos/knowledge_graph_151x151.png\"/\u003e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data loading\n\nThe first step in the step in the data cleansing and profiling is to load the data from the source, in apropriate format. Loading the data from csv file stored in HDFS system."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val newsFilePath \u003d \"us_equities_news_dataset.csv\"\n\nval rawDF \u003d spark.read\n  .option(\"header\", \"true\")\n  .option(\"multiLine\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .option(\"escape\", \"\\\"\")\n  .csv(newsFilePath)\n\n\nrawDF.cache()\nz.show(rawDF, 5)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "println(s\"Total columns : ${rawDF.columns.length}\") \nprintln(s\"Total rows : ${rawDF.count()}\") \n\nrawDF.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions.{col,when, count}\nimport org.apache.spark.sql.{Column, SparkSession}\n\ndef countCols(columns:Array[String]):Array[Column]\u003d{\n    columns.map(c\u003d\u003e{\n      count(when(col(c).isNull,c)).alias(c)\n    })\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "println(\"Count of null entries in the columns : \")\nz.show(rawDF.select(countCols(rawDF.columns):_*))"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val nullRemovedDF \u003d rawDF.filter($\"content\".isNotNull)\n                         .withColumnRenamed(\"article_id\",\"articleId\")\n                         .withColumnRenamed(\"release_date\",\"releaseDate\")\n\nprintln(s\"Total columns : ${nullRemovedDF.columns.length}\") \nprintln(s\"Total rows : ${nullRemovedDF.count()}\") \nprintln(\"Count of null entries in the columns : \")\n\nz.show(nullRemovedDF.select(countCols(nullRemovedDF.columns):_*))"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val cleanDF \u003d nullRemovedDF.withColumn(\"content\", trim(col(\"content\")))\n                           .where(length($\"content\") \u003e\u003d length($\"title\") )\n                           .where(length($\"content\") \u003e 10)\n                           .withColumn(\"content\", regexp_replace($\"content\", \"\\t\", \" \"))\n        \nz.show(cleanDF)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Handling duplicate values\n\nNext task is to remove duplicate news from the dataset, thus counting distinct news and removing duplicates if any."
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dropDisDF \u003d cleanDF.dropDuplicates(\"content\")\n\nprintln(s\"Total columns : ${dropDisDF.columns.length}\") \nprintln(s\"Total rows : ${dropDisDF.count()}\") "
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val countByPublishers \u003d dropDisDF.groupBy(\"provider\").count().sort(col(\"count\").desc)\n\nprintln(s\"The total number of distinct news providers covered by this dataset : ${countByPublishers.count()}\")\nprintln(\"Top 10 news providers in this dataset are : \")\n\ncountByPublishers.show(10, false)\nz.show(countByPublishers.limit(10), 10)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val correctDateDF \u003d dropDisDF.withColumn(\"releaseDate\", to_timestamp(col(\"releaseDate\")))\nz.show(correctDateDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "correctDateDF.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val financialQuarterDF \u003d correctDateDF.withColumn(\"quarter\",quarter(correctDateDF.col(\"releaseDate\")))\n                                      .withColumn(\"year\",year(correctDateDF.col(\"releaseDate\")))\n                                      .select(concat($\"year\", lit(\"Q\"), $\"quarter\") as \"financialQuarter\")\n                                      .groupBy(\"financialQuarter\").count()\n                                      .sort(\"financialQuarter\")\nz.show(financialQuarterDF)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val maximumCountForTitleProviderArticle \u003d correctDateDF.groupBy(\"title\", \"provider\", \"articleId\").count().select(max(\"count\"))\nmaximumCountForTitleProviderArticle.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(correctDateDF)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import java.util.Properties\r\nimport edu.stanford.nlp.pipeline.StanfordCoreNLP\r\nimport edu.stanford.nlp.ling.CoreAnnotations\r\nimport edu.stanford.nlp.neural.rnn.RNNCoreAnnotations\r\nimport edu.stanford.nlp.sentiment.SentimentCoreAnnotations\r\nimport scala.collection.JavaConverters._\r\nimport org.apache.spark.SparkContext\r\nimport edu.stanford.nlp.util.CoreMap"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\n \r\n def sentiment(tweets: String): String \u003d {\r\n    var mainSentiment \u003d 0\r\n    var longest \u003d 0;\r\n    val sentimentText \u003d Array(\"Very Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very Positive\")\r\n    val props \u003d new Properties();\r\n    props.setProperty(\"annotators\", \"tokenize, ssplit, parse, sentiment\");\r\n    new StanfordCoreNLP(props).process(tweets).get(classOf[CoreAnnotations.SentencesAnnotation]).asScala.foreach((sentence: CoreMap) \u003d\u003e {\r\n      val sentiment \u003d RNNCoreAnnotations.getPredictedClass(sentence.get(classOf[SentimentCoreAnnotations.SentimentAnnotatedTree]));\r\n      val partText \u003d sentence.toString();\r\n      if (partText.length() \u003e longest) {\r\n        mainSentiment \u003d sentiment;\r\n        longest \u003d partText.length();\r\n      }\r\n    })\r\n    sentimentText(mainSentiment)\r\n  }\r\n  \r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types._\r\n\r\ndef mysentiment \u003d udf((x: String) \u003d\u003e \r\n    {\r\n        sentiment(x)\r\n    }\r\n    )"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "sentiment(\"Why Shares of Chinese Electric Car Maker NIO Are Flying High Today,news, What s happenin Shares of Chinese electric car maker NIO  NYSE NIO  were sharply higher on Wednesday morning after a Chinese business news outlet reported that the cash strapped company had secured new financing from a major automaker  As of 12 p m  EST  NIO s American depositary shares  ADS  were up about 16  from Tuesday s closing price  So what If this report is accurate and if the deal closes  then it s extremely bullish for NIO\" )"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df3 \u003d correctDateDF.groupBy(\"ticker\",\"releaseDate\").count()\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(df3)"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df3.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dfWithWeekNumber \u003d df3.withColumn(\"dayOfWeek\", date_format(col(\"releaseDate\"), \"E\"))\r\n\r\nz.show(dfWithWeekNumber)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val noNulls \u003d dfWithWeekNumber.na.drop(\"any\")\n\nz.show(noNulls)\nnoNulls.printSchema"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Shifting the news published on Saturday and Sunday to the next buisness day as per stock market working hours."
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df4 \u003d dfWithWeekNumber.withColumn(\"shiftedDate\", when( col(\"dayOfWeek\") \u003d\u003d\u003d \"Sat\", date_add(col(\"releaseDate\"),2))\r\n.when(col(\"dayOfWeek\") \u003d\u003d\u003d \"Sun\", date_add(col(\"releaseDate\"),1))\r\n.otherwise(col(\"releaseDate\")))\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df4.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val checkiIfWork \u003d df4.filter(col(\"releaseDate\") \u003d!\u003d col(\"shiftedDate\"))"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Checking if the date shifting to next buisness day is working"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(checkiIfWork)"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df4.write.option(\"header\",true).csv(\"/tmp/news_data_cleaned/news_features_final\")"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(df4)"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val tsla \u003d df4.filter(col(\"ticker\") \u003d\u003d\u003d \"TSLA\")"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val plottslaNews \u003d tsla.select(col(\"shiftedDate\"), col(\"count\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val plottslaNewsSorted \u003d plottslaNews.sort(col(\"shiftedDate\"))\nz.show(plottslaNewsSorted)"
    }
  ]
}
{
  "metadata": {
    "name": "WatchTrade(BDAD_PROJECT)",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Stock Market Data Cleaning, Profiling and Exploration\n\nWe\u0027re going to perform cleansing, profiling and exploratory data analysis on the real stock market data. We will be using the Huge Stock Market Data from [NYSE, NASDAQ, and NYSE MKT](https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs?datasetId\u003d4538\u0026sortBy\u003dvoteCount).\n\n\u003cimg src\u003d\"https://storage.googleapis.com/kaggle-datasets-images/4538/7213/0ef205a10621870d2d873557864474ff/dataset-cover.jpg?t\u003d2017-11-17-03-48-42\"/\u003e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Source Data Folder Exploration\n\nIn this step, we will explore the data folder to understand the organization of the data physically. This will also give us the insights into how should be write data reader."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val dataFolder \u003d sc.wholeTextFiles(\"loudacre/Stocks\")"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dataFolder.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "dataFolder.take(5).foreach(println)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Reading Data\n\nIn this step, we will read all of the data from the source as DataFrame. Since the data is CSV, we will try to ensure that we infer the schema of the source data."
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val input_path \u003d \"loudacre/Stocks\"\nval input_data \u003d spark.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"multiLine\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .option(\"escape\", \"\\\"\")\n  .load(input_path)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(input_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "input_data.columns.length"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "input_data.printSchema"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "| Column    | Definition \n| :---------| :--------------------------------------------------------------------|\n| Date      | Date is for the date of trading day.                                 |\n| Open      | Open stands for open price on the trading date.                      |\n| High      | High stands for high price on the trading date.                      |\n| Low       | Low stands for low price on the trading date.                        |\n| Close     | Close stands for clsoe price on the trading date.                    |\n| Volume    | Volume stands for the trading volume of shares on the trading date.  |\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Summary And Statistics\n\nIn this step, we will try to get some insights into the statistics of data columns such as min, max, avg, etc. We will also try to understand Inter Quartile Range of the columns."
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val summary \u003d input_data.describe()\nz.show(summary)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(input_data.summary())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Data Cleaning\n\nIn this step, we will perform data cleaning, we will try to find rows, columns and values that don\u0027t align with the data format. We will perform several steps to ensure that the data is standardized."
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.DataFrame\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types._\r\nimport scala.collection.mutable.ArrayBuffer\r\n\r\ndef getNullCount(df: DataFrame): DataFrame \u003d {\r\n    val df_columns \u003d df.columns\r\n    val count_buffer \u003d ArrayBuffer[(String, Long)]()\r\n    for(cur_column \u003c- df_columns) {\r\n        val nullCondition \u003d col(cur_column).isNull \r\n        val nullCount \u003d df.select(col(cur_column)).filter(nullCondition).count()  \r\n        count_buffer.append((cur_column, nullCount))\r\n    }\r\n    val null_df \u003d spark.createDataFrame(count_buffer).toDF(\"column\", \"null_count\")\r\n    null_df\r\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def getNanCount(df: DataFrame): DataFrame \u003d {\r\n    val df_columns \u003d df.columns\r\n    val count_buffer \u003d ArrayBuffer[(String, Long)]()\r\n    for(cur_column \u003c- df_columns) {\r\n        val nanCondition \u003d col(cur_column).isNaN\r\n        val nanCount \u003d df.select(col(cur_column)).filter(nanCondition).count() \r\n        count_buffer.append((cur_column, nanCount))\r\n    }\r\n    val nan_df \u003d spark.createDataFrame(count_buffer).toDF(\"column\", \"nan_count\")\r\n    nan_df\r\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def getBlankCount(df: DataFrame): DataFrame \u003d {\r\n    val df_columns \u003d df.columns\r\n    val count_buffer \u003d ArrayBuffer[(String, Long)]()\r\n    for(cur_column \u003c- df_columns) {\r\n        val blankCount \u003d df.select(col(cur_column)).filter(col(cur_column) \u003d\u003d\u003d \" \").count()\r\n        count_buffer.append((cur_column, blankCount))\r\n    }\r\n    val blank_df \u003d spark.createDataFrame(count_buffer).toDF(\"column\", \"blank_count\")\r\n    blank_df\r\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def getZeroCount(df: DataFrame): DataFrame \u003d {\r\n    val df_columns \u003d df.columns\r\n    val count_buffer \u003d ArrayBuffer[(String, Long)]()\r\n    for(cur_column \u003c- df_columns) {\r\n        val nullCondition \u003d col(cur_column).isNull \r\n        val zeroCount \u003d df.select(col(cur_column)).filter(col(cur_column) \u003d\u003d\u003d 0).count()\r\n        count_buffer.append((cur_column, zeroCount))\r\n    }\r\n    val count_df \u003d spark.createDataFrame(count_buffer).toDF(\"column\", \"zero_count\")\r\n    count_df\r\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val null_count \u003d getNullCount(input_data)\nval nan_count \u003d getNanCount(input_data)\nval blank_count \u003d getBlankCount(input_data)\nval zero_count \u003d getZeroCount(input_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "var merged_df\u003dnull_count.join(nan_count, Seq(\"column\"))\r\nmerged_df\u003dmerged_df.join(blank_count, Seq(\"column\"))\r\nmerged_df\u003dmerged_df.join(zero_count, Seq(\"column\"))\r\nz.show(merged_df)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Basic Feature Engineering\n\nIn this step, we will perform some basic feature engineering to explore the potential features for our future model training."
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val new_df \u003d input_data.withColumn(\"Price\", \n            (col(\"High\") + col(\"Low\") + col(\"Open\") + col(\"Close\"))/4)\nz.show(new_df)"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val new_df_vol \u003d new_df.withColumn(\"PriceVolume\", \n            (col(\"Price\") * col(\"Volume\")))\nz.show(new_df_vol)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val new_input_file \u003d new_df_vol.withColumn(\"filename\", input_file_name)\nz.show(new_input_file)"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val fileStripping \u003d udf((input: String) \u003d\u003e {\n    val temp \u003d input.split(\"/\")(7)\n    temp.split(\"\\\\.\")(0)\n})"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val final_input \u003d new_input_file.withColumn(\"Label\", fileStripping(col(\"filename\"))).drop(\"filename\")\nz.show(final_input)"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(final_input.describe())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exploratory Data Analysis\n\nIn this set, we will do exploratory data analysis on the features present and handengineered, we will perform univariate and some multivariate analysis to undertand features and their importance."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Univariate Data Analysis\n\nIn this step, we will start with univariate feature analysis. We will mostly perform all exploration and analysis on one feature for now called PriceVolume."
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val exp_data \u003d final_input.select(\"Date\", \"PriceVolume\", \"Label\")\nz.show(exp_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val (startValues,counts) \u003d exp_data.select(\"PriceVolume\").map(value \u003d\u003e value.getDouble(0)).rdd.histogram(50)\nval zippedValues \u003d startValues.zip(counts)\ncase class HistRow(startPoint:Double,count:Long)\nval rowRDD \u003d zippedValues.map( value \u003d\u003e HistRow(value._1,value._2))\nval histDf \u003d spark.createDataFrame(rowRDD)\nz.show(histDf)"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "exp_data.select(skewness(col(\"PriceVolume\"))).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "exp_data.select(kurtosis(col(\"PriceVolume\"))).show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "The above data distribution for feature, its skewness and kurtosis clearly indicate that the feature needs some engineering. It is very skewed and the data distribution is far from normal. In the steps following, we will be applying some functions to standardize and scale the given feature."
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val logged_data \u003d final_input.withColumn(\"PriceVolumeLog\",\n                            log(col(\"PriceVolume\"))\n                            )\nz.show(logged_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val null_cnts \u003d getNullCount(logged_data)\nz.show(null_cnts)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "We can clearly see that after applying log, there are null values introduced, and so we need to filter those null values before further processing."
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val logged_fil_data \u003d logged_data.filter($\"PriceVolumeLog\".isNotNull)\nval (startValues,counts) \u003d logged_fil_data.select(\"PriceVolumeLog\").map(value \u003d\u003e value.getDouble(0)).rdd.histogram(50)"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val zippedValues \u003d startValues.zip(counts)\r\ncase class HistRow(startPoint:Double,count:Long)\r\nval rowRDD \u003d zippedValues.map( value \u003d\u003e HistRow(value._1,value._2))\r\nval histDf \u003d spark.createDataFrame(rowRDD)\r\nz.show(histDf)"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val fil \u003d exp_data.where($\"Label\" \u003d\u003d\u003d \"ge\")\nz.show(fil)"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val year_data \u003d fil.withColumn(\"Year\",\n                date_format(col(\"Date\"), \"y\")\n)\nz.show(year_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val group_data \u003d year_data.groupBy(\"Year\")\n        .agg(\n            sum(\"PriceVolume\").as(\"Vel\")\n            )\nz.show(group_data)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Mutlivariate Analysis\n\nIn this step we will perform multivariate data analysis, we will perform bivariate analysis between features and try rto understand correlation between them."
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.mllib.stat.{MultivariateStatisticalSummary, Statistics}\nStatistics.corr(logged_fil_data.select(\"Price\").rdd.map(x\u003d\u003e x.getDouble(0)), logged_fil_data.select(\"PriceVolumeLog\").rdd.map(x\u003d\u003e x.getDouble(0)))"
    }
  ]
}
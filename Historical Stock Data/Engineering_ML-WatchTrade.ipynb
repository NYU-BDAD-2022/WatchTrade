{
  "metadata": {
    "name": "WatchTrade(Part2)",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Stock Market Advanced Indicator Engineering And Learning\n\nWe\u0027re going to perform cleansing, profiling and exploratory data analysis on the real stock market data. We will be using the Huge Stock Market Data from [NYSE, NASDAQ, and NYSE MKT](https://www.kaggle.com/datasets/borismarjanovic/price-volume-data-for-all-us-stocks-etfs?datasetId\u003d4538\u0026sortBy\u003dvoteCount).\n\n\u003cimg src\u003d\"https://storage.googleapis.com/kaggle-datasets-images/4538/7213/0ef205a10621870d2d873557864474ff/dataset-cover.jpg?t\u003d2017-11-17-03-48-42\"/\u003e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Reading Preprocessed Data From HDFS\n\nIn this step, we will read all of the preprocessedd data stored as parquet from the HDFS. Since the data is parquet, we will try to ensure that we infer the schema of the source data."
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val input_path \u003d \"loudacre/Project/hist_stock_datapre.parquet\"\nval input_data \u003d spark.read.format(\"parquet\").load(input_path)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(input_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "input_data.columns.length"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "input_data.printSchema"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Machine Learning\n\nIn this step, we will perform basic machine engineering to explore the potential models for label prediction."
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def charr \u003d udf((st:String)\u003d\u003e{\n    st.substring(0,1)\n})"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window\nvar df \u003d input_data\ndf \u003d df.withColumn(\"rank\", percent_rank().over(Window.partitionBy().orderBy(\"date\")))\ndf \u003d df.withColumn(\"label_new\", charr(col(\"Label\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val train_data \u003d df.where(\"rank \u003c\u003d .8\").drop(\"rank\")\nval test_data \u003d df.where(\"rank \u003e .8\").drop(\"rank\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "print(\"train size \u003d \", train_data.count)\nprint(\"test size \u003d \", test_data.count)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(train_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.feature.VectorAssembler\n\nval vecAssembler \u003d new VectorAssembler()\n  .setInputCols(Array(\"Open\", \"Volume\"))\n  .setOutputCol(\"features\")\n  \nval vecTrainDF \u003d vecAssembler.transform(train_data)\nz.show(vecTrainDF.select(\"features\", \"Close\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.regression.LinearRegression\n\nval lr \u003d new LinearRegression()\n  .setFeaturesCol(\"features\")\n  .setLabelCol(\"Close\")"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val lrModel \u003d lr.fit(vecTrainDF)\nval m \u003d lrModel.coefficients(0)\nval b \u003d lrModel.intercept"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.Pipeline\nval pipeline \u003d new Pipeline().setStages(Array(vecAssembler, lr))\nval pipelineModel \u003d pipeline.fit(train_data)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val predDF \u003d pipelineModel.transform(test_data)\npredDF.select(\"features\", \"Close\", \"prediction\").show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(predDF.limit(100).toDF())"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.evaluation.RegressionEvaluator\nval regressionEvaluator \u003d new RegressionEvaluator()\n .setPredictionCol(\"prediction\")\n .setLabelCol(\"Close\")\n .setMetricName(\"rmse\")\nval rmse \u003d regressionEvaluator.evaluate(predDF)\nprintln(f\"RMSE is $rmse%.1f\")\n\nval r2 \u003d regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\nprintln(s\"R2 is $r2\")"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\nval categoricalCols \u003d train_data.dtypes.filter{ case (field, dataType) \u003d\u003e\n field \u003d\u003d \"label_new\"}.map(_._1)\nval indexOutputCols \u003d categoricalCols.map(_ + \"_index\")\nval oheOutputCols \u003d categoricalCols.map(_ + \"_OHE\")\n\nval stringIndexer \u003d new StringIndexer()\n .setInputCols(categoricalCols)\n .setOutputCols(indexOutputCols)\n .setHandleInvalid(\"skip\")\n \nval oheEncoder \u003d new OneHotEncoder()\n .setInputCols(indexOutputCols)\n .setOutputCols(oheOutputCols)\n\nval numericCols \u003d train_data.dtypes.filter{ case (field, dataType) \u003d\u003e\n field \u003d\u003d \"Open\" || field \u003d\u003d \"Volume\"}.map(_._1)\n\nval assemblerInputs \u003d oheOutputCols ++ numericCols\nval vecAssembler \u003d new VectorAssembler()\n .setInputCols(assemblerInputs)\n .setOutputCol(\"features\")"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val lr \u003d new LinearRegression()\n .setLabelCol(\"Close\")\n .setFeaturesCol(\"features\")\nval pipeline \u003d new Pipeline()\n .setStages(Array(stringIndexer, oheEncoder, vecAssembler, lr))"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val pipelineModel \u003d pipeline.fit(train_data)\n \nval predDF \u003d pipelineModel.transform(test_data)\npredDF.select(\"features\", \"Close\", \"prediction\").show(5)\n\nval rmse \u003d regressionEvaluator.evaluate(predDF)\nprintln(f\"RMSE is $rmse%.1f\")\n\nval r2 \u003d regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\nprintln(s\"R2 is $r2\")"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(predDF.limit(100).toDF())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Advanced Technical Indicator Engineering\n\nIn this step, we will perform advanced Technical Indicator engineering to explore the potential Technical Indicator to show on dashboard."
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val exp_data \u003d input_data.select(\"Date\", \"Close\")\nval fil \u003d exp_data.where($\"Label\" \u003d\u003d\u003d \"bgr\")\nz.show(fil)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Technical Indicators\n\nIn this step, we will perform advanced feature engineering to explore the potential features for our future model training."
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.DataFrame\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types._\r\nimport scala.collection.mutable.ArrayBuffer"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val w \u003d org.apache.spark.sql.expressions.Window.orderBy(\"Date\")  \nimport org.apache.spark.sql.functions.lag\nval xy \u003d input_data.where($\"Label\" \u003d\u003d\u003d \"odp\").select(\"Date\", \"Close\")\nxy.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val mm \u003d xy.withColumn(\"Close_new\", lag(\"Close\", 1, 0).over(w))\nmm.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val momentum \u003d mm.withColumn(\"momentum_1d\",\n    (col(\"Close\") - col(\"Close_new\"))\n    )\nmomentum.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "\r\nimport scala.collection.mutable.WrappedArray\r\nval rsi \u003d udf((values: WrappedArray[Double])\u003d\u003e {\r\n    val up_temp \u003d values.filter(_ \u003e 0)\r\n    val down_temp \u003d values.filter(_ \u003c 0)\r\n    val up: Double \u003d up_temp.sum/up_temp.length\r\n    var down: Double \u003d -1* (down_temp.sum/down_temp.length)\r\n    if(down_temp.length \u003d\u003d 0){\r\n       down \u003d 0.0 \r\n    }\r\n    // print(\"up \u003d \", up)\r\n    // print(\"down \u003d \", down)\r\n    100 * up/(up+down)\r\n})"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.expressions.WindowSpec\nval df_rsi_temp \u003d momentum.withColumn(\"RSI_IM\", ((collect_list(\"momentum_1d\"))\n            .over(org.apache.spark.sql.expressions.Window.orderBy(col(\"Date\")).rowsBetween(-14,0))))\nz.show(df_rsi_temp)"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df_rsi \u003d df_rsi_temp.withColumn(\"RSI\", \n                rsi(col(\"RSI_IM\")))\nz.show(df_rsi)"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df_rsi.limit(100).toDF().select(\"Close\", \"Close_new\", \"momentum_1d\", \"RSI\").show()"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def bbands()\u003d{\n    val df_bb_temp \u003d df_rsi.withColumn(\"BB_IM\", ((collect_list(\"momentum_1d\"))\n            .over(org.apache.spark.sql.expressions.Window.orderBy(col(\"Date\")).rowsBetween(-20,0))))\n    df_bb_temp\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val bb_df \u003d bbands()\nz.show(bb_df)"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import scala.collection.mutable.WrappedArray\r\nval bband_avg \u003d udf((values: WrappedArray[Double])\u003d\u003e {\r\n    values.sum/values.length\r\n})\r\nval bband_std \u003d udf((a: WrappedArray[Double]) \u003d\u003e {\r\n    val mean \u003d a.sum / a.length\r\n    val squareErrors \u003d a.map(x \u003d\u003e x - mean).map(x \u003d\u003e x * x)\r\n    math.sqrt(squareErrors.sum / a.length)\r\n})"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df_bb_avg \u003d bb_df.withColumn(\"BB_AVG\", \n                bband_avg(col(\"BB_IM\")))\nval df_bb_std \u003d df_bb_avg.withColumn(\"BB_STD\", \n                bband_std(col(\"BB_IM\")))\nz.show(df_bb_std)"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df_bb_lower \u003d df_bb_std.withColumn(\"BB_Lower_Band\", \n                (col(\"BB_AVG\") - (col(\"BB_STD\")*20))).drop(\"RSI_IM\").drop(\"BB_IM\")\nz.show(df_bb_lower)"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df_bb_upper \u003d df_bb_lower.withColumn(\"BB_Upper_Band\", \n                (col(\"BB_AVG\") + (col(\"BB_STD\")*20))).drop(\"RSI_IM\").drop(\"BB_IM\")\nz.show(df_bb_upper)"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df_bb \u003d df_bb_upper.withColumn(\"BB_Middle_Band\", \n                (col(\"BB_AVG\"))).orderBy(desc(\"Date\"))\nz.show(df_bb)"
    }
  ]
}